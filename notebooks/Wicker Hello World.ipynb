{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3901f76c",
   "metadata": {},
   "source": [
    "# Wicker Hello World\n",
    "\n",
    "In this tutorial we demonstrate how to write and read a dataset given only a S3 bucket as infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c0899",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Install wicker with the spark plugin into your a new virtual environment.\n",
    "For this tutorial you should also have numpy and pillow installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ac6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wicker[spark], numpy, pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35e5a1",
   "metadata": {},
   "source": [
    "Download and untar the cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239847b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl and untar the Cifar dataset\n",
    "# curl https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz --output cifar10.tgz\n",
    "# tar -xvf cifar10.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1ae68",
   "metadata": {},
   "source": [
    "### Writing the Dataset\n",
    "\n",
    "This simple dataset consists of 10 classes and 60k images partitioned into 50k train and 10k test. \n",
    "Each image is 32,32,3 so we can define the dataset schema with a string field for the label and a numpy field for the image. \n",
    "Note that we need to provide a unique identifier for each sample, in most real-world cases we might supply something like a tuple of the timestamp of collection \n",
    "and the id of the collection device. Here we can just generate an arbitrary UUID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667de8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker import schema\n",
    "\n",
    "DATASET_SCHEMA = schema.DatasetSchema(\n",
    "    fields=[\n",
    "        schema.StringField(\"label\", description=\"ground truth label of our image\"),\n",
    "        schema.StringField(\"id\", description=\"uniqe id of data sample\"),\n",
    "        schema.NumpyField(\"image\", shape=(-1, -1, 3), dtype=\"uint8\", description=\"image as a numpy array\"),\n",
    "    ],\n",
    "    primary_keys=[\"label\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71b55c",
   "metadata": {},
   "source": [
    "Additionally we supply a name and version for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cifar10\"\n",
    "DATASET_VERSION = \"0.0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdeda5d",
   "metadata": {},
   "source": [
    "Next let's organize the data so we can more easily persist it in our dataset. \n",
    "This dataset is relatively small (<200 Mb) so we can just read everything into local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7501e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# replace default path to cifar10 data as needed\n",
    "DEFAULT_DATA_PATH = \"/code/cifar10\" #os.getcwd()+'/cifar10'\n",
    "\n",
    "if not os.path.isdir(DEFAULT_DATA_PATH): \n",
    "    raise OSError(f\"cifar10 directory does not exist at {os.getcwd()}\")\n",
    "\n",
    "path = os.walk(DEFAULT_DATA_PATH)\n",
    "dataset = []\n",
    "for root, directories, files in path:\n",
    "    for file in files:\n",
    "        root_split = root.split('/')\n",
    "        partition = root_split[-2]\n",
    "        label = root_split[-1]\n",
    "        im = np.asarray(Image.open(root+'/'+file))\n",
    "        dataset.append({'partition_name': partition, 'raw_data':{'label': label, 'id': str(uuid.uuid4()), 'image': im}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a624c",
   "metadata": {},
   "source": [
    "In this example we'll use the spark plugin and a local spark cluster to persist and shuffle our data. <br>\n",
    "To use the spark API you will just need to supply an RDD containing:\n",
    "\n",
    "1. The dataset partition\n",
    "2. A dictionary of the data to be persisted\n",
    "\n",
    "i.e. pyspark.rdd.RDD[Tuple[str, Dict[str, Any]]]\n",
    "\n",
    "Let's look at the example below using a local spark cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker.plugins.spark import persist_wicker_dataset\n",
    "from pyspark.sql import SparkSession\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a local spark session\n",
    "spark_session = SparkSession.builder.appName(\"test\").master(\"local[*]\")\n",
    "spark = spark_session.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD containing partition name and raw data\n",
    "# note that if you are running a local spark cluster you may \n",
    "# increase the number of partitions so your stages don't become too big.\n",
    "# here we use 256\n",
    "rdd = sc.parallelize(copy.deepcopy(dataset), numSlices=512)\n",
    "data_rdd = rdd.map(lambda data_dict: data_dict[\"raw_data\"])\n",
    "partition_name_rdd = rdd.map(lambda data_dict: data_dict[\"partition_name\"])\n",
    "partition_rdd = partition_name_rdd.zip(data_rdd)\n",
    "partition_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282694e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_wicker_dataset(\n",
    "    DATASET_NAME,\n",
    "    DATASET_VERSION,\n",
    "    DATASET_SCHEMA,\n",
    "    partition_rdd,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447912fa",
   "metadata": {},
   "source": [
    "If successful, the output of the previous cell should be `{'train': 50000, 'test': 10000}` <br>\n",
    "signifying that 50k examples were written to the train partition and 10k to the test partition\n",
    "\n",
    "That's it! Our data has been shuffled and is now in our S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7af7b",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Now let's read examples from the train partition of our dataset\n",
    "Note that we can load subsets of the data, we don't actually care about the id so let's forget about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051664b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker.core.datasets import S3Dataset\n",
    "\n",
    "# initialize dataset with subset of data columns\n",
    "ds = S3Dataset(DATASET_NAME, DATASET_VERSION, \"train\", columns_to_load=['label', 'image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x0 = ds[0]\n",
    "# Accessing the first element hits s3 \n",
    "# and loads data into memory O(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = ds[1]\n",
    "# access to contiguous indices is fast O(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x2 = ds[25000]\n",
    "# accessing a sample \"far away\" results in a missed cache hit\n",
    "# and data is loaded from s3\n",
    "\n",
    "# the number of rows contiguous rows loaded from storage is configurable and \n",
    "# can be tuned by the user based on specific training parameters like hardware resources and size of each training example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3136a6-c580-402e-8e29-213601eeb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first 16 images from our dataset\n",
    "from PIL import Image\n",
    "for i in range(16):\n",
    "    display(Image.fromarray(ds[i]['image']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
