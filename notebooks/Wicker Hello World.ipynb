{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3901f76c",
   "metadata": {},
   "source": [
    "# Wicker Hello World\n",
    "\n",
    "In this tutorial we demonstrate how to write and read a dataset given only a S3 bucket as infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c0899",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Install wicker with the spark plugin into your a new virtual environment.\n",
    "For this tutorial you should also have numpy and pillow installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ac6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wicker[spark], numpy, pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35e5a1",
   "metadata": {},
   "source": [
    "Download and untar the cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239847b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl and untar the Cifar dataset\n",
    "# curl https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz --output cifar10.tgz\n",
    "# tar -xvf cifar10.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1ae68",
   "metadata": {},
   "source": [
    "### Writing the Dataset\n",
    "\n",
    "This simple dataset consists of 10 classes and 60k images partitioned into 50k train and 10k test. \n",
    "Each image is 32,32,3 so we can define the dataset schema with a string field for the label and a numpy field for the image. Note that we would also have defined an object field or ... for the image field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667de8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker import schema\n",
    "\n",
    "DATASET_SCHEMA = schema.DatasetSchema(\n",
    "    fields=[\n",
    "        schema.StringField(\"label\", description=\"ground truth label of our image\"),\n",
    "        schema.NumpyField(\"image\", shape=(-1, -1, 3), dtype=\"uint8\", description=\"image as a numpy array\"),\n",
    "    ],\n",
    "    primary_keys=[\"label\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71b55c",
   "metadata": {},
   "source": [
    "Additionally we supply a name and version for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cifar10\"\n",
    "DATASET_VERSION = \"0.0.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdeda5d",
   "metadata": {},
   "source": [
    "Next let's organize the data so we can more easily persist it in our dataset. \n",
    "This dataset is relatively small (<200 Mb) so we can just read everything into local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7501e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "path = os.walk('/code/cifar10')\n",
    "dataset = []\n",
    "\n",
    "for root, directories, files in path:\n",
    "    for file in files:\n",
    "        root_split = root.split('/')\n",
    "        partition = root_split[-2]\n",
    "        label = root_split[-1]\n",
    "        im = np.asarray(Image.open(root+'/'+file))\n",
    "        dataset.append({'partition_name': partition, 'raw_data':{'label': label, 'image': im}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a624c",
   "metadata": {},
   "source": [
    "In this example we'll use the spark plugin and a local spark cluster to persist and shuffle our data.\n",
    "To use the spark API you will just need to supply an RDD containing:\n",
    "\n",
    "1. The dataset partition\n",
    "2. A dictionary of the data to be persisted\n",
    "\n",
    "i.e. pyspark.rdd.RDD[Tuple[str, Dict[str, Any]]]\n",
    "\n",
    "Let's look at the example below using a local spark cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker.plugins.spark import persist_wicker_dataset\n",
    "from pyspark.sql import SparkSession\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a local spark session\n",
    "spark_session = SparkSession.builder.appName(\"test\").master(\"local[*]\")\n",
    "spark = spark_session.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD containing partition name and raw data\n",
    "# note that if you are running a local spark cluster you will \n",
    "# likely have to play with the number of partitions so your stages don't become too big.\n",
    "# here we use 256\n",
    "rdd = sc.parallelize(copy.deepcopy(dataset), numSlices=256)\n",
    "data_rdd = rdd.map(lambda data_dict: data_dict[\"raw_data\"])\n",
    "partition_name_rdd = rdd.map(lambda data_dict: data_dict[\"partition_name\"])\n",
    "partition_rdd = partition_name_rdd.zip(data_rdd)\n",
    "partition_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282694e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_wicker_dataset(\n",
    "    DATASET_NAME,\n",
    "    DATASET_VERSION,\n",
    "    DATASET_SCHEMA,\n",
    "    partition_rdd,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447912fa",
   "metadata": {},
   "source": [
    "That's it! Our data has been shuffled and is now in our S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7af7b",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Now let's read from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051664b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker.core.datasets import S3Dataset\n",
    "\n",
    "ds = S3Dataset(DATASET_NAME, DATASET_VERSION, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first read O(seconds)\n",
    "%%time\n",
    "x0 = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access to contiguous indices is fast\n",
    "%%time\n",
    "x1 = ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in fact entire train dataset was loaded into memory since it was small\n",
    "# by default chunks of X Mb are loaded at a time\n",
    "%%time\n",
    "x2 = ds[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
