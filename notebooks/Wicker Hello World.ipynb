{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3901f76c",
   "metadata": {},
   "source": [
    "# Wicker Hello World\n",
    "\n",
    "In this tutorial we demonstrate how to write and read a dataset given only a S3 bucket as infrastructure. <br>\n",
    "You will need to specify your S3 configuration via the `.wickerconfig.json` file (see [Getting Started](https://github.com/woven-planet/wicker/blob/main/docs/source/getstarted.rst) for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c0899",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "Install wicker with the spark plugin into your a new virtual environment.\n",
    "For this tutorial you should also have numpy and pillow installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ac6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wicker[spark] numpy pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35e5a1",
   "metadata": {},
   "source": [
    "Download and untar the cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239847b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl and untar the Cifar dataset\n",
    "# !curl https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz --output cifar10.tgz\n",
    "# !tar -xvf cifar10.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1ae68",
   "metadata": {},
   "source": [
    "### Writing the Dataset\n",
    "\n",
    "The Cifar-10 dataset consists of 60k images with 10 possible labels partitioned into 50k train and 10k test sets. <br>\n",
    "Each image is 32,32,3 so we can define the dataset schema with a string field for the label and a numpy field for the image. \n",
    "<br>  We also need to provide primary key(s) using the field values. The primary key tuple should be a unique identifier for the example over the dataset. In most real-world cases we might supply something like a tuple of the timestamp of collection and the id of the collection device, here we can just generate an arbitrary UUID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667de8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker import schema\n",
    "\n",
    "DATASET_SCHEMA = schema.DatasetSchema(\n",
    "    fields=[\n",
    "        schema.StringField(\"label\", description=\"ground truth label of our image\"),\n",
    "        schema.StringField(\"id\", description=\"uniqe id of data sample\"),\n",
    "        schema.NumpyField(\"image\", shape=(-1, -1, 3), dtype=\"uint8\", description=\"image as a numpy array\"),\n",
    "    ],\n",
    "    primary_keys=[\"label\", \"id\"], # tuple must be unique over the dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71b55c",
   "metadata": {},
   "source": [
    "Additionally we supply a name and version for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361e3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"cifar10\"\n",
    "DATASET_VERSION = \"0.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdeda5d",
   "metadata": {},
   "source": [
    "Next let's organize the data so we can more easily persist it in our dataset. \n",
    "This dataset is relatively small (<200 Mb) so we can just read everything into local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7501e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# replace with default path to cifar10\n",
    "DEFAULT_DATA_PATH = '/code/cifar10' #os.getcwd()+'/cifar10'\n",
    "\n",
    "if not os.path.isdir(DEFAULT_DATA_PATH): \n",
    "    raise OSError(f\"cifar10 directory does not exist at {os.getcwd()}\")\n",
    "\n",
    "path = os.walk(DEFAULT_DATA_PATH)\n",
    "dataset = []\n",
    "for root, directories, files in path:\n",
    "    if files:\n",
    "        root_split = root.split('/')\n",
    "        partition = root_split[-2] # train, test folders\n",
    "        label = root_split[-1] # images for each label type are in a folder with that label name\n",
    "        print(f'{partition}, {label}')\n",
    "    for file in files:\n",
    "        im = np.asarray(Image.open(root+'/'+file))\n",
    "        # Tuple(partition, sample data dictionary)\n",
    "        dataset.append((partition, \n",
    "                        {'label': label, 'id': str(uuid.uuid4()), 'image': im}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a624c",
   "metadata": {},
   "source": [
    "In this example we'll use the spark plugin and a local spark cluster to persist and shuffle our data. <br>\n",
    "To use the spark API you will just need to supply an RDD containing:\n",
    "\n",
    "1. The dataset partition\n",
    "2. A dictionary of the data to be persisted\n",
    "\n",
    "i.e. pyspark.rdd.RDD[Tuple[str, Dict[str, Any]]]\n",
    "\n",
    "Let's look at the example below using a local spark cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d0285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker.plugins.spark import SparkPersistor\n",
    "from pyspark.sql import SparkSession\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a local spark session\n",
    "spark_session = SparkSession.builder.appName(\"test\").master(\"local[*]\")\n",
    "spark = spark_session.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9381e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RDD containing partition name and raw data\n",
    "# note that if you are running a local spark cluster you may \n",
    "# increase the number of partitions so your stages don't become too big.\n",
    "# here we use 256\n",
    "rdd = sc.parallelize(copy.deepcopy(dataset), numSlices=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282694e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistor = SparkPersistor()\n",
    "persistor.persist_wicker_dataset(\n",
    "    DATASET_NAME,\n",
    "    DATASET_VERSION,\n",
    "    DATASET_SCHEMA,\n",
    "    rdd,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447912fa",
   "metadata": {},
   "source": [
    "If successful, the output of the previous cell should be `{'train': 50000, 'test': 10000}` <br>\n",
    "signifying that 50k examples were written to the train partition and 10k to the test partition\n",
    "\n",
    "That's it! Our data has been shuffled and is now in our S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a078c",
   "metadata": {},
   "source": [
    "Now we want to log the data to our data registry/versioning system. We internally use weights\n",
    "and biases so this is all we support at current but please add more extension PRs.\n",
    "\n",
    "Use need to set the following config parameters either in your wickerconfig.json or in your \n",
    "environment to contact the right W&B endpoint with the api key. And we need to install the wandb plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# ENV:\n",
    "os.environ['WANDB_API_KEY'] = 'your_key'\n",
    "os.environ['WANDB_BASE_URL'] = 'your_base_url'\n",
    "\n",
    "# wicker config\n",
    "{\n",
    "    \"wandb_config\":\n",
    "    {\n",
    "        \"wandb_api_key\": \"your_key\"\n",
    "        \"wandb_base_url\": \"your_base_url\"\n",
    "    }\n",
    "}\n",
    "\n",
    "pip install wicker[wandb]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5e421",
   "metadata": {},
   "source": [
    "We can then run the command with the appropriate input to save our data to our versioning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker.plugins.wandb import version_dataset\n",
    "\n",
    "# this takes the name, the version, the entity to store under (we use perception owner to store our data)\n",
    "# a dict of metadata key value pairs, and a data backend (we currently only support s3)\n",
    "version_dataset(DATASET_NAME, DATASET_VERSION, \"perception\", {\"notebook\": \"hello_world\"}, \"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4c977",
   "metadata": {},
   "source": [
    "Now the dataset is versioned on the cloud through W&B!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7af7b",
   "metadata": {},
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Now let's read examples from the train partition of our dataset\n",
    "Note that we can load subsets of the data, we don't actually care about the id so let's forget about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051664b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wicker.core.datasets import S3Dataset\n",
    "\n",
    "DATASET_NAME = \"cifar10\"\n",
    "DATASET_VERSION = \"0.0.1\"\n",
    "\n",
    "# initialize dataset with subset of data columns\n",
    "ds = S3Dataset(DATASET_NAME, DATASET_VERSION, \"train\", columns_to_load=['label', 'image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x0 = ds[0]\n",
    "# Accessing the first element hits s3 \n",
    "# and loads data into memory O(100 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = ds[1]\n",
    "# access to contiguous indices is fast O(1 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x2 = ds[25000]\n",
    "# accessing a sample not in downloaded block results \n",
    "# in a cache miss and data is loaded from s3\n",
    "\n",
    "# the number contiguous rows loaded from storage is configurable and \n",
    "# can be tuned by the user based on specific training parameters like hardware resources and size of each training example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9d65b-869f-4653-92fa-fbff66d75e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib if not already in your environment\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "for i in range(16):\n",
    "    idx=random.randint(0, 50000) \n",
    "    img = Image.fromarray(ds[idx]['image'])\n",
    "    label = ds[idx]['label']\n",
    "    fig.add_subplot(4, 4, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.title(label)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365d3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
